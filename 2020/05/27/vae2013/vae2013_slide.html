<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Luyiyun" />
  <title>论文精读-VAE-2013</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="https://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
  <script src="https://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
<link rel="alternate" href="/atom.xml" title="卢一云的博客" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
<body>
<div class="slide titlepage">
  <h1 class="title">论文精读-VAE-2013</h1>
  <p class="author">
Luyiyun
  </p>
  <p class="date">2020-05-27 15:04:19</p>
</div>
<div class="slide" id="TOC">

</div>
<div id="auto-encoding-variational-bayes" class="slide section level1">
<h1>Auto-Encoding Variational Bayes</h1>
<ul>
<li>杂志: None</li>
<li>IF: None</li>
<li>分区: None</li>
</ul>
</div>
<div class="slide section level1">

<h2 id="introduction">Introduction</h2>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li><p>对continuous latent variable进行概率密度估计时，会存在一个难以处理的后验分布。variational bayesian（VB，其最常见的形式是mean-field方法）可以进行近似优化。但正常的mean-field方法需要后验分布期望的解析解。</p></li>
<li><p>本研究介绍了一种参数估计方法，其将variational lower bound给参数化，从而可以通过标准的梯度下降算法（SGVB）进行训练。</p></li>
<li><p>为了能够针对iid的样本集得到其continuous latent variables，本研究在SGVB的基础上进一步提出了一种AutoEncoding VB（AEVB）算法。其应用SGVB估计方法在一个recogntion model上，可以非常有效率地得到后验估计，而不用进行非常昂贵的iterative inference方法（如MCMC）。</p>
<p>学习的近似后验推理模型可用于许多任务，例如识别、去噪、表示和可视化目的。</p>
<p>当使用的recognition model是NN时，即variational auto-encoder。</p></li>
</ol>
</div>
<div class="incremental">
<p><em>这里对背景进行一下介绍</em></p>
</div>
</div>
<div class="slide section level1">

<h3 id="先验和后验分布">1. 先验和后验分布</h3>
<p>在bayes统计中，有先验和后验的概念：</p>
<p><span class="math display">\[
p(H=h|X) = \frac{p(X|H=h)p(H=h)}{\int{p(X|H=s)p(H=s)}ds} \tag{1}
\]</span></p>
<p>其中<span class="math inline">\(p(H|X)\)</span>为后验分布，<span class="math inline">\(p(H)\)</span>为先验分布，<span class="math inline">\(p(X|H)\)</span>为似然函数，如果是有多个iid样本的话，其一般表示为<span class="math inline">\(\prod_{i=1}^N{p(X=x_i|H)}\)</span>。bayes统计的关键在于如何在已知先验、似然函数的基础上来得到后验。以上计算式中，最麻烦的在于分母中的积分项难以得到：</p>
<ul>
<li>对于高维的<span class="math inline">\(H\)</span>，我们无法通过近似离散方法得到。</li>
<li>而对于比较复杂的似然函数和先验分布形式，我们也无法得到精确解（甚至有时候似然函数和先验分布的精确形式也无法得到）。</li>
</ul>
<p>所以必须使用一些inference的方法来进行推断。</p>
</div>
<div class="slide section level1">

<h3 id="隐变量模型">2. 隐变量模型</h3>
<p>典型的隐变量模型有：GMM（高斯混合模型）、HMM（隐马尔可夫）等。</p>
<p>实际上，一个隐变量模型（机器学习意义上的）也可以看做是一个求后验的问题。</p>
<p>我们有显变量<span class="math inline">\(X\)</span>，其分布依赖于一些隐变量<span class="math inline">\(H\)</span>，由隐变量得到显变量的关系式为<span class="math inline">\(p(X|H)\)</span>，如果这个关系式未知，则需要使用参数化的函数<span class="math inline">\(p_{\theta}(X|H)\)</span>来表示。我们一般需要做两个任务：</p>
<ul>
<li>inference：已知<span class="math inline">\(X=x_i\)</span>，其对应的隐变量的分布是什么：<span class="math inline">\(p(H|X=x_i)\)</span>。</li>
<li>learning：关系式未知时，我们是否能够将参数学习到：<span class="math inline">\(\theta^*=\argmin_{\theta}{\log{p_{\theta}(X)}}\)</span>（极大似然法，注意不是极大化<span class="math inline">\(p(X|H)\)</span>，是极大化的边际似然）。</li>
</ul>
<p>显然，对于第一个问题，上面的bayes统计中的问题是一样的，隐变量即先验，显变量即后验，这个<span class="math inline">\(p(X|H)\)</span>即似然函数。（我们可以根据我们的领域知识为隐变量给定一个合适的先验分布，或者使用一个平坦的、无信息的先验分布。）</p>
<p>至于第二个问题，如果我们能够知道后验的话，也是非常简单的：</p>
<p><span class="math display">\[\log{p_{\theta}(X)} = \log{p_{\theta}(X|H)}+\log{p(H)}-\log{p_{\theta}(H|X)} \tag{2}\]</span></p>
<p>（当然，实际上好像也没人这样搞。。。）</p>
<p>总之，在隐变量模型中，进行后验的推断也是非常重要的。</p>
</div>
<div class="slide section level1">

<h3 id="mcmc">3. MCMC</h3>
<p>MCMC是进行后验推断的最重要的方法，特别是在bayes统计领域（<em>MCMC拯救了bayes估计</em>）。</p>
<p>为什么MCMC能够进行进行后验估计？我们需要先简单了解一下MCMC的过程（这里是最简单的Metropolis算法，Metropolis-Hastings算法和他类似）:</p>
<div class="incremental">
<blockquote>
<p>target distribution：<span class="math inline">\(p(X)\)</span><br />
proposed distribution：<span class="math inline">\(q(X)\)</span>，任意分布</p>
<p>第一步：随机一个初始值<span class="math inline">\(X=x_0\)</span>，只要让<span class="math inline">\(p(X=x_0)\ne0\)</span>即可。</p>
<p>第二步：使用<span class="math inline">\(q(X)\)</span>得到一个随机的点<span class="math inline">\(x&#39;_i\)</span>。</p>
<p>第三步：计算transition probability： <span class="math display">\[p_{move}=\min(\frac{p(x&#39;_i)}{p(x_i)}) \tag{3}\]</span> 其中<span class="math inline">\(p(x_i)\)</span>是当前所在的点。依据此概率来判断是否接受<span class="math inline">\(x&#39;_i\)</span>作为<span class="math inline">\(x_{i+1}\)</span>，否则<span class="math inline">\(x_{i+1}=x_{i}\)</span>。</p>
</blockquote>
</div>
<div class="incremental">
<p>现在我们的目标分布是<span class="math inline">\(p(H|X)\)</span>，当把它代入上面的transition probability的计算的时候，我们发现，<strong>分母上的积分式被约掉了</strong>。这时候不需要进行烦人的积分运算了，我们只需要能够计算似然和先验即可。</p>
<p>这也就是为什么MCMC可以进行后验推断的原因。</p>
<p>现实应用的时候，Metropolis或Metropolis-Hastings算法效率非常低，需要使用Gibbs算法或Hamilton算法来替代，但本质上原理是一样的。</p>
</div>
<div class="incremental">
<p>MCMC的优点：</p>
<ul>
<li>无偏估计</li>
<li>适用性强，只需要能够写出似然函数即可</li>
</ul>
<p>MCMC的缺点：</p>
<ul>
<li>太慢了</li>
<li>采集的样本也不能直接用，必须要做一些处理，比如降低其自相关性、预烧期等等</li>
</ul>
</div>
</div>
<div class="slide section level1">

<h3 id="变分推断">4. 变分推断</h3>
<p>这是后验推断的另一种思路，即使用一个可变化的（或者参数化的）分布（<span class="math inline">\(q_{\phi}(H)\)</span>）去逼近后验分布。</p>
<p>由上面的公式<span class="math inline">\((2)\)</span>，我们可以得到：</p>
<p><span class="math display">\[\log{p(X)} = \log{\frac{p(X|H)}{q_{\phi}(H)}}+\log{p(H)}-\log{\frac{p(H|X)}{q_{\phi}(H)}}\]</span></p>
<p>等式两边对<span class="math inline">\(q_{\phi}(H)\)</span>做期望，得到：</p>
<p><span class="math display">\[\log{p(X)}=-D_{KL}(q_{\phi}(H)|p(X|H))+E_{q_{\phi}}[\log{p(H)}]+D_{KL}(q_{\phi}(H)|p(H|X)) \tag{4}\]</span></p>
<p>等式左边是关于<span class="math inline">\(\phi\)</span>不变的，所以如果希望最小化<span class="math inline">\(q_{\phi}(H)\)</span>和后验的“距离”–<span class="math inline">\(D_{KL}(q_{\phi}(H)|p(H|X))\)</span>，只需要最大化下面的东西即可，这个就是<strong>ELBO(evidence lower bound)</strong>：</p>
<p><span class="math display">\[\phi^*=argmax_{\phi}{ELBO}=argmax_{\phi}\{-D_{KL}(q_{\phi}(H)|p(X|H))+E_{q_{\phi}}[\log{p(H)}]\} \tag{5}\]</span></p>
<p><em>以上结果也可以通过Jessen不等式得到</em></p>
<div class="incremental">
<p>现在问题就变成了如何最小化ELBO，这时候，<span class="math inline">\(q_{\phi}(H)\)</span>的选择就变得格外重要了，其必须要满足以下两个方面：</p>
<ul>
<li>足够简单，可以保证ELBO可以计算出来（注意到，这里有求期望的过程，所以可以使用Monte Carlo方法来估计，这时候需要可以对<span class="math inline">\(q_{\phi}(H)\)</span>采样）。</li>
<li>足够复杂，能够比较好的拟合后验分布。</li>
</ul>
<p>这时候，大家传统的选择是mean-field，即使用相互独立的多个分布组成的高维分布来做<span class="math inline">\(q_{\phi}(H)\)</span>（最最常用的当然是每个成分用gaussian）。</p>
</div>
</div>
<div class="slide section level1">

<h3 id="mean-field">5. mean-field</h3>
<p>但需要注意，mean-field需要似然函数必须是简单的，最好是指数分布簇中的。不然去显式的计算ELBO依然是困难的。</p>
<p><em>显式的计算出ELBO意味着mean-field没有使用Monte Carlo方法去估计期望。而VAE是用MC去估计的期望，从这一部分来说，mean-field是有优势的。而且mean-field可以改为Monte Carlo版本，从而能够对任意的似然函数进行估计，但灵活性上可能会不如VAE（毕竟是神经网络）。</em></p>
</div>
<div class="slide section level1">

<h2 id="methods">Methods</h2>
<div class="incremental">
<h3 id="问题">1. 问题</h3>
<p>现在将讨论的范围限制在下面的区域内：</p>
<ol style="list-style-type: decimal">
<li>每个iid样本都有一个对应的latent variables；</li>
<li>我们使用maximum likelihood（ML）和 maximum a posteriori（MAP）来估计参数；</li>
<li>使用变分推断来得到latent variables。</li>
</ol>
<p>数据集为<span class="math inline">\(X=\{x^{(i)}\}_{i=1}^N\)</span>，假设数据有某个未观测到的连续变量<span class="math inline">\(z\)</span>生成，其有两个步骤：</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(z\)</span>从某个先验的分布<span class="math inline">\(p_{\theta^*}(z)\)</span>得到；</li>
<li><span class="math inline">\(x^{(i)}\)</span>由条件概率<span class="math inline">\(p_{\theta^*}(x|z)\)</span>得到。</li>
</ol>
<p>另外假设这两个分布都是可以参数化的，而且对于参数是可导的。在此之外，不再增加任何的假设。</p>
<div class="incremental">
<p>所以我们会面临两个问题：</p>
<div>
<ul class="incremental">
<li>Intractability：边际似然<span class="math inline">\(p(X)\)</span>不好算、后验分布不好算、似然函数很复杂（比如是一个NN）导致传统的mean-field也用不了。</li>
<li>large dataset：大数据，所以batch optimization无法做到，凡是基于采样的方法也无法有效的进行。</li>
</ul>
</div>
<p>我们希望得到的解决方法具有下面的特点：</p>
<div>
<ul class="incremental">
<li>能够进行高效地ML或MAP，估计<span class="math inline">\(\theta\)</span>，从而可以进行数据生成；</li>
<li>能够高效地估计给定<span class="math inline">\(x\)</span>的后验分布<span class="math inline">\(z\)</span>，从而可以对数据进行编码或表示学习；</li>
<li>能够得到<span class="math inline">\(x\)</span>的边际估计，从而可以进行降噪、补全或超分辨率操作。</li>
</ul>
</div>
<p>为了能够得到上面的特性，这里引入一个recognition model<span class="math inline">\(q_{\phi}(z|x)\)</span>，来逼近真实后验。需要注意到，这里的<span class="math inline">\(q_{\phi}(z|x)\)</span>不需要有mean-field的独立性假设和closed-form expectation，其可以是一个NN。</p>
<p><em>现在我们希望得到的是每个<span class="math inline">\(x^{(i)}\)</span>都可以有个<span class="math inline">\(z^{(i)}\)</span>，而不是<span class="math inline">\(p(z|\{x^{(i)}\}_{i=1}^N)\)</span>，这实际上是一般bayes统计中希望得到的后验。所以现在我们希望估计的后验依赖于<span class="math inline">\(x\)</span>，则我们使用的<span class="math inline">\(q\)</span>也依赖于<span class="math inline">\(x\)</span>。</em></p>
<p>从编码理论上来看，<span class="math inline">\(q_{\phi}(z|x)\)</span>可以看做是一个encoder，而<span class="math inline">\(p_{\theta}(x|z)\)</span>可以看做是一个decoder。</p>
</div>
</div>
</div>
<div class="slide section level1">

<h3 id="变分下界">2. 变分下界</h3>
<p><img src="vae2013_2020-05-27-18-36-47.png" alt="变分下界推导" /><br />
</p>
<div class="incremental">
<p>和上面普通的变分推断过程的区别：</p>
<ul>
<li>可变化的分布现在依赖于<span class="math inline">\(x\)</span>。</li>
<li>我们现在希望使用Monte Carlo梯度估计来解决问题。</li>
</ul>
<p>但使用Monte Carlo梯度估计需要从带有参数<span class="math inline">\(\phi\)</span>的分布中采样，这会使得结果对于<span class="math inline">\(\phi\)</span>不可导。</p>
<blockquote>
<p>当然，还有另外的办法，也可以使用RL中的方法（<code>pyro</code>的<a href="http://pyro.ai/examples/svi_part_iii.html" target="_blank" rel="noopener">tutorial</a>中给出了一个推导）：</p>
<p><span class="math display">\[
\begin{aligned}
  \nabla_{\phi} \mathbb{E}_{q_{\phi}(\mathbf{z})}\left[f_{\phi}(\mathbf{z})\right]&amp;=\nabla_{\phi} \int d \mathbf{z} q_{\phi}(\mathbf{z}) f_{\phi}(\mathbf{z})\\
  &amp;=\int d \mathbf{z}\left\{\left(\nabla_{\phi} q_{\phi}(\mathbf{z})\right) f_{\phi}(\mathbf{z})+q_{\phi}(\mathbf{z})\left(\nabla_{\phi} f_{\phi}(\mathbf{z})\right)\right\} \\
  &amp;=\mathbb{E}_{q_{\phi}(\mathbf{z})}\left[\left(\nabla_{\phi} \log q_{\phi}(\mathbf{z})\right) f_{\phi}(\mathbf{z})+\nabla_{\phi} f_{\phi}(\mathbf{z})\right]
\end{aligned}
\]</span></p>
<p>其中</p>
<p><span class="math display">\[\nabla_{\phi} q_{\phi}(\mathbf{z})=q_{\phi}(\mathbf{z}) \nabla_{\phi} \log q_{\phi}(\mathbf{z})\]</span></p>
<p>但这个的问题在于得到的估计的方差太大，很难使用，要不然就得采大量的样本来降低方差。针对此问题，本研究提出了一个更加有针对性的策略–即重参数化技巧。</p>
</blockquote>
</div>
</div>
<div class="slide section level1">

<h3 id="sgvb估计器和aevb算法">3. SGVB估计器和AEVB算法</h3>
<p>这里会产生两个版本的SGVB estimator：</p>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li><p>第一个版本，使用重参数化技巧对文中的公式2进行处理，得到：</p>
<p>利用后面会介绍的reparameterization trick，可以将<span class="math inline">\(\phi\)</span>从期望里面移出来：</p>
<p><span class="math display">\[\widetilde{\mathbf{z}}=g_{\phi}(\boldsymbol{\epsilon}, \mathbf{x}) \quad with \quad \boldsymbol{\epsilon} \sim p(\boldsymbol{\epsilon})\]</span></p>
<p>然后就可以使用Monte Carlo方法估计期望：</p>
<p><span class="math display">\[\mathbb{E}_{q_{\phi}\left(\mathbf{z} | \mathbf{x}^{(i)}\right)}[f(\mathbf{z})]=\mathbb{E}_{p(\boldsymbol{\epsilon})}\left[f\left(g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}, \mathbf{x}^{(i)}\right)\right)\right] \simeq \frac{1}{L} \sum_{l=1}^{L} f\left(g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}^{(l)}, \mathbf{x}^{(i)}\right)\right) \quad where \quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon})\]</span></p>
<p>只需要使用这个技术，即得到了我们的SGVB estimator：</p>
<p><span class="math display">\[\begin{aligned} \widetilde{\mathcal{L}}^{A}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right) &amp;=\frac{1}{L} \sum_{l=1}^{L} \log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)}, \mathbf{z}^{(i, l)}\right)-\log q_{\boldsymbol{\phi}}\left(\mathbf{z}^{(i, l)} | \mathbf{x}^{(i)}\right) \\ \text { where } \quad \mathbf{z}^{(i, l)} &amp;=g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}^{(i, l)}, \mathbf{x}^{(i)}\right) \quad \text { and } \quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon}) \end{aligned}\]</span></p></li>
<li><p>第二个版本，对第一个版本的进一步细化。如果我们能够将文中3式中的KL散度显式的写出来，则我们只需要对后面那一项使用采样估计即可：</p>
<p><span class="math display">\[\widetilde{\mathcal{L}}^{B}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)=-D_{K L}\left(q_{\boldsymbol{\phi}}\left(\mathbf{z} | \mathbf{x}^{(i)}\right) \| p_{\boldsymbol{\theta}}(\mathbf{z})\right)+\frac{1}{L} \sum_{l=1}^{L}\left(\log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} | \mathbf{z}^{(i, l)}\right)\right) \\
 \text{where} \quad \mathbf{z}^{(i, l)}=g_{\boldsymbol{\phi}}\left(\boldsymbol{\epsilon}^{(i, l)}, \mathbf{x}^{(i)}\right) \quad \text{and} \quad \boldsymbol{\epsilon}^{(l)} \sim p(\boldsymbol{\epsilon}) \tag {6}\]</span></p></li>
</ol>
</div>
<div class="incremental">
<p><em>SGVB是用来指示如何计算和估计ELBO的</em></p>
</div>
<div class="incremental">
<p>我们能够估计ELBO之后，那做极大化边际似然了：</p>
<p><span class="math display">\[\mathcal{L}(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{X}) \simeq \widetilde{\mathcal{L}}^{M}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{X}^{M}\right)=\frac{N}{M} \sum_{i=1}^{M} \widetilde{\mathcal{L}}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right)\]</span></p>
<p>其中<span class="math inline">\(M\)</span>是每个minibatch使用的样本的数量，<span class="math inline">\(N\)</span>表示对于每个样本进行Monte Carlo计算期望使用的采样数量，在实验中发现，<span class="math inline">\(N=1\)</span>就有不错的效果。整个过程可以通过SGG系列算法进行。</p>
</div>
<div class="incremental">
<blockquote>
<p>这里，我的理解：正常的步骤，首先第一步更新<span class="math inline">\(\phi\)</span>最大化ELBO，则此时使得后验的估计是准确的。在后验估计准确的情况下，ELBO等于边际似然。第二部，更新<span class="math inline">\(\theta\)</span>最大化ELBO，此时等价于最大化边际似然。以上两个步骤重复进行，类似EM。但我们又可以知道，如果交替训练可以找到最优点，则不交替训练也一定能找到最优点，所以我们直接一起训练反而能够找到更好的结果。</p>
</blockquote>
<blockquote>
<p>在AEs的角度来看式6，我们能够看到其每一项的意义。<br />
第一项是一个regularization，用来尽量保证后验和先验相同；<br />
第二项是一个negative reconstruction loss。<br />
即整个ELBO的含义在于，在保证后验估计和先验尽量相似的情况下，最小化重建误差；或者是在尽量缩小重建误差的情况下，最小化后验估计和先验的差距就能得到准确的后验估计。<br />
</p>
<p><em>是不是有点类似岭回归的自编码器版本 :)</em></p>
</blockquote>
</div>
</div>
<div class="slide section level1">

<h3 id="重参数化技巧">4. 重参数化技巧</h3>
<p>其需要解决的问题：我们计算梯度的过程中需要进行采样，采样的分布上有我们需要估计的参数，则梯度流无法通过“采样”这个过程，所以无法进行估计。</p>
<p>解决方法：把需要采样的那个分布<span class="math inline">\(z\sim q_{\phi}(z|x)\)</span>表示成<span class="math inline">\(z=g_{\phi}(\epsilon, x)\)</span>，这样参数就被移出了采样过程。</p>
<p>示例：现在我们需要采样进行期望估计的分布为<span class="math inline">\(z \sim p(z | x)=\mathcal{N}\left(\mu, \sigma^{2}\right)\)</span>，则我们可以表示为<span class="math inline">\(z=\mu+\sigma\epsilon\)</span>，其中<span class="math inline">\(\epsilon\sim\mathcal{N}(0,1)\)</span>，这时我们发现实际上z还是服从原来的分布的，则我们会有下面的期望估计的变化：</p>
<p><span class="math display">\[\mathbb{E}_{\mathcal{N}\left(z ; \mu, \sigma^{2}\right)}[f(z)]=\mathbb{E}_{\mathcal{N}(\epsilon ; 0,1)}[f(\mu+\sigma \epsilon)] \simeq \frac{1}{L} \sum_{l=1}^{L} f\left(\mu+\sigma \epsilon^{(l)}\right) \\ \text { where } \epsilon^{(l)} \sim \mathcal{N}(0,1)\]</span></p>
<div class="incremental">
<p>要能够使用重参数化技巧，意味着<span class="math inline">\(q_{\phi}(z|x)\)</span>必须满足一定的条件：</p>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li>可以知道<span class="math inline">\(q_{\phi}(z|x)\)</span>的inverse CDF。我们假设这个inverse CDF为<span class="math inline">\(g_{\phi}(\epsilon, x)\)</span>，则取<span class="math inline">\(\epsilon\sim\mathcal{U}(0,I)\)</span>即可。比如：Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions</li>
<li>像gaussian一样，其参数是location scale的，即表示的是平移、缩放。则可以表示<span class="math inline">\(g(.)=location+scale\cdot\epsilon\)</span>，其中<span class="math inline">\(\epsilon\)</span>服从那个在location=0、scale=1的那个分布。比如：Laplace, Elliptical, Student’s t, Logistic, Uniform, Triangular and Gaussian distributions</li>
<li>可以分解为不同的容易处理的组分的。比如：Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions</li>
</ol>
</div>
</div>
</div>
<div class="slide section level1">

<h3 id="变分自编码器">5. 变分自编码器</h3>
<p><em>以上相对于为研究人员提供了一个框架。其中还没有假设的是<span class="math inline">\(p(z)\)</span>、<span class="math inline">\(p(z|x)\)</span>和<span class="math inline">\(p(z|x)\)</span>的形式。</em></p>
<p>以下便是最常用的一种：</p>
<p><img src="vae2013_2020-05-28-00-04-21.png" alt="AEVB算法" /><br />
</p>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li>认为latent variables的prior是独立的标准正态分布。
<ul class="incremental">
<li>了解贝叶斯统计的可以知道，如果样本量够大，样本的影响会基本彻底淹没先验的影响，这时候先验取什么对后验的影响是非常小的；</li>
<li>这个有点像岭回归，相对于只是起一个对隐变量的微小约束。如果没有这个约束，隐变量可以取任意的值，这显然不合理，而且估计的方差也会很大。从岭回归的角度，这相当于限制隐变量估计的方差。</li>
<li>对于一个没有任何其他知识的隐变量的估计，我们预先的认为其不会很大（这就是标准正态先验的含义）是合理的。</li>
</ul></li>
<li><span class="math inline">\(p_{\theta}(x|z)\)</span>是经过NN输出参数的Gaussian或Bernoulli。</li>
<li>认为<span class="math inline">\(p_{\theta}(z|x)\)</span>是经过NN输出参数的独立Gaussian。
<ul class="incremental">
<li>2和3的含义即认为确定的<span class="math inline">\(x\)</span>或<span class="math inline">\(z\)</span>其对应的<span class="math inline">\(z\)</span>或<span class="math inline">\(x\)</span>依然是个分布，但是一个在预测值周围震荡的gaussian或Bernoulli（如果NN换成线性，不就是OLS和Logistic嘛）。</li>
</ul></li>
</ol>
</div>
<div class="incremental">
<p>对于<span class="math inline">\(q_{\phi}(x|z)\)</span>的选择就随意多了，我们让它和<span class="math inline">\(p_{\theta}(x|z)\)</span>有相同的形式。所以在上面的2假设成立的情况下，则我们会得到最优解。如果上面的2假设不成立，那么我们也不会有太大的损失（比较前面有NN，使得我们能够拟合成一个Gaussian的机会大大增加了）(<em>不好理解就类比线性模型</em>)。</p>
<p>经过一阵推导，得到下面的loss公式：</p>
<p><span class="math display">\[\mathcal{L}\left(\boldsymbol{\theta}, \boldsymbol{\phi} ; \mathbf{x}^{(i)}\right) \simeq \frac{1}{2} \sum_{j=1}^{J}\left(1+\log \left(\left(\sigma_{j}^{(i)}\right)^{2}\right)-\left(\mu_{j}^{(i)}\right)^{2}-\left(\sigma_{j}^{(i)}\right)^{2}\right)+\frac{1}{L} \sum_{l=1}^{L} \log p_{\boldsymbol{\theta}}\left(\mathbf{x}^{(i)} | \mathbf{z}^{(i, l)}\right)\]</span></p>
<p><span class="math display">\[\text { where } \mathbf{z}^{(i, l)}=\boldsymbol{\mu}^{(i)}+\boldsymbol{\sigma}^{(i)} \odot \boldsymbol{\epsilon}^{(l)} \quad \text { and } \quad \boldsymbol{\epsilon}^{(l)} \sim \mathcal{N}(0, \mathbf{I})\tag{7}\]</span></p>
<p>loss的第二项根据数据的形式（continuous or binary）选择Gaussian或Bernoulli的形式。</p>
</div>
</div>
<div class="slide section level1">

<h2 id="related-work">Related work</h2>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li>wake-sleep算法【HDFN95】是目前为止已知的和本研究一样general的continuous latent variables建模方法。其缺点是需要2-steps的optimization（有两个目标函数）。优点是可以应用于discrete latent variables。其计算复杂度和AEVB相当。</li>
<li>在Stochastic variational inference【HBWP13】的领域，一些研究通过限制一些技巧可以控制gradient estimator的方差【BJP12】、【RGB13】，而【SK13】使用了类似的重参数化技巧。</li>
<li>之前已经有相关研究【Row98】，探索PCA和ML的结合，相对于是使用linear AEs。</li>
<li>AEs方面的最新研究【VLL+10】，试图去最小化<span class="math inline">\(X\)</span>和<span class="math inline">\(Z\)</span>之间的MI，但这个regularization不如本文的方法自然；【BTL13】使用Markov Chain的样本来训练noisy的AEs。</li>
<li>【GMW13】、【RMW14】和本文类似的结果，但各研究之间是独立进行的。</li>
</ol>
</div>
</div>
<div class="slide section level1">

<h2 id="experiments">Experiments</h2>
<p>在MNIST和Frey Face数据集上进行了实验，并和其他算法进行了比较。</p>
<p>在decoder的输出层是sigmoid，使之归一化到0-1。参数采样自<span class="math inline">\(\mathcal{N}(0, 0.01)\)</span>，使用Adagrad进行训练。minibatch size是100。对于MNIST，encoder hidden是500；对于Frey Face则是200。</p>
<div>
<ol class="incremental" style="list-style-type: decimal">
<li><p>首先是lower bound的比较，AEVB能够得到更大的lower bound</p>
<p><img src="vae2013_2020-05-28-00-48-14.png" alt="和wake-sleep的比较，看谁能得到更大的lower bound" /><br />
</p></li>
<li><p>和MCMC EM方法、wake-sleep方法比较了一下边际似然，发现AEVB还是好的。</p>
<p><img src="vae2013_2020-05-28-00-58-43.png" alt="边际似然的比较" /><br />
</p></li>
<li><p>高维数据的可视化：</p>
<p><img src="vae2013_2020-05-28-00-59-49.png" /><br />
</p>
<p><img src="vae2013_2020-05-28-01-00-00.png" /><br />
</p></li>
</ol>
</div>
</div>
<div class="slide section level1">

<h2 id="conclusion">Conclusion</h2>
<p>未来：</p>
<ul>
<li>CNNs；</li>
<li>时间序列数据；</li>
<li>应用到global paramters；</li>
<li>应用到supervised models。</li>
</ul>
</div>
<div class="slide section level1">

<h2 id="questions">Questions</h2>
<ol style="list-style-type: decimal">
<li>什么叫做“global parameters”？</li>
</ol>
</div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
